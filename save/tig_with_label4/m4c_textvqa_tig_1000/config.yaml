dataset_attributes:
  m4c_textvqa:
    data_root_dir: /home/ubuntu/hzy/pythia/data
    fast_read: False
    features_max_len: 100
    image_depth_first: False
    image_features:
      test:
      - /home/ubuntu/hzy/pythia/data/textvqa_gcy/test,m4c_textvqa_ocr_en_frcn_features/test_images
      train:
      - /home/ubuntu/hzy/pythia/data/textvqa_gcy/train,m4c_textvqa_ocr_en_frcn_features/train_images
      val:
      - /home/ubuntu/hzy/pythia/data/textvqa_gcy/train,m4c_textvqa_ocr_en_frcn_features/train_images
    imdb_files:
      test:
      - imdb/m4c_textvqa/imdb_test_ocr_en.npy
      train:
      - imdb/m4c_textvqa/imdb_train_ocr_en.npy
      val:
      - imdb/m4c_textvqa/imdb_val_ocr_en.npy
    processors:
      answer_processor:
        params:
          context_preprocessor:
            params:
              
            type: simple_word
          max_copy_steps: 12
          max_length: 50
          num_answers: 10
          preprocessor:
            params:
              
            type: simple_word
          vocab_file: m4c_vocabs/textvqa/fixed_answer_vocab_textvqa_5k.txt
        type: m4c_answer
      bbox_processor:
        params:
          max_length: 50
        type: bbox
      context_processor:
        params:
          max_length: 50
          model_file: /home/ubuntu/hzy/pythia//pythia/models/.vector_cache/wiki.en.bin
        type: fasttext
      copy_processor:
        params:
          max_length: 100
        type: copy
      obj_context_processor:
        params:
          max_length: 100
          model_file: /home/ubuntu/hzy/pythia//pythia/models/.vector_cache/wiki.en.bin
        type: fasttext
      ocr_token_processor:
        params:
          
        type: simple_word
      phoc_processor:
        params:
          max_length: 50
        type: phoc
      text_processor:
        params:
          max_length: 20
        type: bert_tokenizer
    return_info: True
    use_ocr: True
    use_ocr_info: True
datasets: m4c_textvqa
log_foldername: m4c_textvqa_tig_1000
model: tig
model_attributes:
  tig:
    classifier:
      ocr_max_num: 50
      ocr_ptr_net:
        hidden_size: 768
        query_key_size: 768
      params:
        
      type: linear
    gat:
      dropout_prob: 0.1
      num_gat_heads: 8
      num_gat_layers: 1
    losses:
    - type: m4c_decoding_bce_with_mask
    lr_scale_frcn: 0.1
    lr_scale_mmt: 1.0
    lr_scale_text_bert: 0.1
    metrics:
    - type: textvqa_accuracy
    mmt:
      hidden_size: 768
      num_hidden_layers: 4
    model: tig
    model_data_dir: /home/ubuntu/hzy/pythia/data
    obj:
      dropout_prob: 0.1
      mmt_in_dim: 2048
      semantic_mmt_in_dim: 300
    ocr:
      dropout_prob: 0.1
      mmt_in_dim: 3002
      semantic_mmt_in_dim: 904
      visual_mmt_in_dim: 2048
    text_bert:
      num_hidden_layers: 3
    text_bert_init_from_bert_base: True
optimizer_attributes:
  params:
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
  type: Adam
training_parameters:
  batch_size: 128
  clip_gradients: True
  clip_norm_mode: all
  data_parallel: True
  dataset_size_proportional_sampling: True
  device: cuda
  distributed: False
  evalai_inference: False
  experiment_name: run
  load_pretrained: False
  local_rank: None
  log_dir: ./logs
  log_interval: 100
  logger_level: info
  lr_ratio: 0.1
  lr_scheduler: True
  lr_steps:
  - 14000
  - 19000
  max_epochs: None
  max_grad_l2_norm: 0.25
  max_iterations: 48000
  metric_minimize: False
  monitored_metric: m4c_textvqa/textvqa_accuracy
  num_workers: 10
  patience: 4000
  pin_memory: False
  pretrained_mapping:
    
  resume: False
  resume_file: None
  run_type: train+inference
  save_dir: save/tig_with_label4
  seed: 1000
  should_early_stop: False
  should_not_log: False
  snapshot_interval: 1000
  task_size_proportional_sampling: True
  trainer: base_trainer
  use_warmup: True
  verbose_dump: False
  warmup_factor: 0.2
  warmup_iterations: 1000